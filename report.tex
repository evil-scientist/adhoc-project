\documentclass[conference]{IEEEtran}
\usepackage{graphicx}
\begin{document}

\title{Ad-Hoc Networks Project Report}

\author{\IEEEauthorblockN{Suryansh Sharma, Jure Vidmar, Suhail Nogd, Eghonghon Eigbe}
\IEEEauthorblockA{Delft University of Technology Intelligent Systems Department\\
The Netherlands}
%\IEEEauthorrefmark{1}
\IEEEauthorblockA{S.sharma-13@student.tudelft.nl\\
XXXXX@student.tudelft.nl\\XXXXX@student.tudelft.nl\\XXXXX@student.tudelft.nl }}
% make the title area
\maketitle

\begin{abstract}
This paper studies whether emotional expressions provided by a human observer can improve the learning performance of a reinforcement learning agent by influencing the exploration/exploitation trade-off in Q learning. When the human observer shows positive affect the agent tends more towards exploitation while negative affect will make the agent explore more.
\end{abstract}

% no keywords

\section{Introduction}

\subsection{Motivation and related work}

\section{Research question}

\subsection{Hypotheses} 

\section{Method}

\subsection{Materials}

\subsubsection{Exploration/Exploitation Strategies}

\subsubsection{EARL: Emotion, Adaptation and Reinforcement Learning Framework}

\begin{enumerate}
    \item Emotion recognition module perceives human facial expressions in real time. For this we will be using Affectiva \cite{affectiva} (more in section \ref{sec:affectiva})
    \item A reinforcement learning agent fed the recognized emotion as input.
    \item An artificial emotion module slot can utilize all available information to produce artificial emotion of agent which can be later used as intrinsic reward or as input for the expression module.
    \item An expression module aims to express robot emotion. This module consist of a robot head with different degrees of freedom(such as eyes, ears, lips and eyelids) to generate expressions.
\end{enumerate}

\subsection{Affectiva} \label{sec:affectiva}
    
\subsection{Experimental setup / approach}
%You describe how the experiment was setup (is applicable). 

\subsubsection{Continuous environment}
%We implement our experiments in the same simulated continuous gridworld of \cite{Joost}, as shown in Fig. \ref{fig_grid}. Details can be found in \cite{Joost}
%\begin{figure}[!t]
%\centering
%\includegraphics[width=0.4\textwidth]{img/grid.png}
%\caption{The experimental gridworld (as used in \cite{Joost}).}
%\label{fig_grid}
%\end{figure}

\subsubsection{Social learning and non-social learning}

\subsection{Measures}
%You probably have to measure something in order to test you hypothesis. This is where you explain what and how you measure this. For example, nice weather is measured as the average temperature over the day and incidents are all by the police reported traffic incidents on registered highways.

%\begin{figure}[!t]
%\centering
%\includegraphics[width=0.4\textwidth]{img/measure.png}
%\caption{The experiment results (from \cite{Joost}.}
%\label{fig_measure}
%\end{figure}

\subsection{Work plan}
%Here you describe how you organize your work over time. This is needed for your proposal, but you remove this section in the final paper.
\begin{itemize}
    \item Week 1.5: Finalize how the expressions will influence the temperature parameter.
    \item Week 1.6: Look further into the implementation details of the work of Broekens \cite{Joost}
    \item Week 1.7: Adapt implementation and prepare experiment
    \item Week 1.8: Perform experiment with human observers
    \item Week 1.9: Finalize results, conlusion and discussion and prepare presentation
    \item Week 1.10: Give presentation
\end{itemize}
\section{Results}
%You describe the results as they are, and how these related to the hypotheses (do they support the hypotheses or not)
\subsection{Discussion}
%You discuss the meaning of the results in more depth also in relation to the work of others introduced in 1.1
\section{Conclusion and further research}
%Your final conclusion and recommendations for further/future work.

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%


% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}

% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals/conferences use top floats
% exclusively. Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the \fnbelowfloat
% command of the stfloats package.

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

\begin{thebibliography}{1}
\bibitem{SocialIntelligence}
Dautenhahn, K. (2007). Socially intelligent robots: dimensions of human–robot interaction. Philosophical Transactions of the Royal Society of London B: Biological Sciences, 362(1480), 679-704.

\bibitem{SocialRobots1}
J. Fasola, M. J. Mataric, “Using socially assistive human-
robot interaction to motivate physical exercise for older
adults.” in Proceedings of the IEEE, vol. 100, no. 8, pp. 2512-
2526, 2012.

\bibitem{SocialRobots2}
P. Baxter et al. “Long-term human-robot interaction with
young users,” in IEEE/ACM HRI 2011 Conference, 2011.

\bibitem{SocialRobots3} 
B. Scassellati, H. Admoni and M. Mataric, “Robots for use in
autism research,” in Annual review of biomedical
engineering, vol. 14, pp. 275-294, 2012.

\bibitem{RLwithhumans}
Thomaz, A. L., \& Breazeal, C. (2006, July). Reinforcement learning with human teachers: Evidence of feedback and guidance with implications for learning performance. In Aaai (Vol. 6, pp. 1000-1005).

\bibitem{Humanintheloop}
Taylor, M. E., \& Borealis, A. I. (2018). Improving Reinforcement Learning with Human Input. In IJCAI (pp. 5724-5728).

\bibitem{BabiesReinforcement}
Messinger, D. S., Duvivier, L. L., Warren, Z., Mahoor, M., Baker, J., Warlaumont, A. S., \& Ruvolo, P. (2014). Affective computing, emotional development, and autism. In The Oxford Handbook of Affective Computing.

\bibitem{Joost}
Broekens, J. (2007). Emotion and reinforcement: affective facial expressions facilitate robot learning. In Artifical intelligence for human computing (pp. 113-132). Springer, Berlin, Heidelberg.

\bibitem{feedbackRL1} 
Breazeal, C., Velasquez, J.: Toward teaching a robot `infant' using emotive communication acts. In: Edmonds, B., Dautenhahn, K. (eds.): Socially Situated Intelligence: a workshop held at SAB'98, Zürich. University of Zürich Technical Report
(1998) 25-40

\bibitem{feedbackRL2} 
Isbell, C. L. Jr., Shelton, C. R., Kearns, M., Singh, S., Stone, P.: A social reinforcement learning agent. In: Proceedings of the fifth international conference on Autonomous
agents. ACM (2001) 377-384

\bibitem{iosifidis2011person} 
Iosifidis, Alexandros and Tefas, Anastasios and Pitas, Ioannis, Person specific activity recognition using fuzzy learning and discriminant analysis. Signal Processing Conference, 2011 19th European. IEEE (pp. 1974-1978)

\bibitem{jordao2018human}
Human Activity Recognition Based on Wearable Sensor Data: A Standardization of the State-of-the-Art. Jordao, Artur and Nazare Jr, Antonio C and Sena, Jessica and Schwartz, William Robson. arXiv preprint arXiv:1806.05226 (2018)

\bibitem{sutton1998reinforcement}
Sutton, Richard S and Barto, Andrew G and others. Reinforcement learning: An introduction. MIT press (1998)

\bibitem{broekens2007affect}
Broekens, Joost and Kosters, Walter A and Verbeek, Fons J. Affect, anticipation, and adaptation: Affect-controlled selection of anticipatory simulation in artificial adaptive agents Adaptive behavior vol. 15 n. 4. Sage Publications Sage UK: London, England (2007). pp(397-422)

\bibitem{Rose}
Rose, Susan A., Lorelle R. Futterweit, and Jeffery J. Jankowski. "The relation of affect to attention and learning in infancy." Child Development 70.3 (1999): 549-559.

\bibitem{Hecker}
von Hecker, Ulrich, and Thorsten Meiser. "Defocused attention in depressed mood: evidence from source monitoring." Emotion 5.4 (2005): 456.

\bibitem{tempQlearning}
Ishida, Fumihiko, et al. "Reinforcement-learning agents with different temperature parameters explain the variety of human action–selection behavior in a Markov decision process task." Neurocomputing 72.7-9 (2009): 1979-1984.

\bibitem{affectiva:metrics}
 "Metrics". Affectiva Developer Portal, 2018, https://developer.affectiva.com/metrics/. Accessed 30 Sept 2018.

\bibitem{affectiva}
McDuff, Daniel, et al. "AFFDEX SDK: a cross-platform real-time multi-face expression recognition toolkit." Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems. ACM, 2016.

\end{thebibliography}

\end{document}
